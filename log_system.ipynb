{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Python libraries\n",
    "!pip install Flask redis kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import redis\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Configuration for the distributed cache\n",
    "# In a real-world scenario, you would use a Redis Cluster or Sentinel for sharding\n",
    "# For this example, we'll simulate sharding by hashing the key\n",
    "REDIS_NODES = [\n",
    "    {'host': 'redis-node-1', 'port': 6379},\n",
    "    {'host': 'redis-node-2', 'port': 6379},\n",
    "]\n",
    "redis_connections = [redis.Redis(**node) for node in REDIS_NODES]\n",
    "\n",
    "def get_redis_node(key):\n",
    "    \"\"\"Simple sharding function based on key hash.\"\"\"\n",
    "    h = int(hashlib.md5(key.encode()).hexdigest(), 16)\n",
    "    return redis_connections[h % len(redis_connections)]\n",
    "\n",
    "def set_with_lru_cache(key, value, expiry=600):\n",
    "    \"\"\"Sets a value in the distributed cache with LRU eviction.\"\"\"\n",
    "    node = get_redis_node(key)\n",
    "    # The built-in Redis keyspace is already a perfect LRU cache with maxmemory-policy.\n",
    "    # We just need to set the key and an expiration.\n",
    "    node.setex(key, expiry, json.dumps(value))\n",
    "\n",
    "def get_from_lru_cache(key):\n",
    "    \"\"\"Retrieves a value from the cache.\"\"\"\n",
    "    node = get_redis_node(key)\n",
    "    value = node.get(key)\n",
    "    if value:\n",
    "        return json.loads(value)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_ingestion_api.py\n",
    "from flask import Flask, request, jsonify\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Kafka producer setup\n",
    "KAFKA_BROKERS = ['kafka-broker-1:9092', 'kafka-broker-2:9092']\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKERS,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "@app.route('/ingest', methods=['POST'])\n",
    "def ingest_log():\n",
    "    log_event = request.get_json()\n",
    "    if not log_event:\n",
    "        return jsonify({\"error\": \"Invalid request\"}), 400\n",
    "\n",
    "    # Add a unique ID to the event\n",
    "    log_event['event_id'] = str(uuid.uuid4())\n",
    "    \n",
    "    # Send the log event to the 'logs' topic\n",
    "    producer.send('logs', log_event)\n",
    "    \n",
    "    return jsonify({\"status\": \"success\", \"message\": \"Log event queued\"}), 202\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_worker.py\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import time\n",
    "from utils import set_with_lru_cache\n",
    "\n",
    "# Mock a database interaction for a sharded B+ tree\n",
    "# In a real system, this would be an ORM call to a database like PostgreSQL\n",
    "def write_to_database(log_event):\n",
    "    \"\"\"Simulates writing to a database shard.\"\"\"\n",
    "    # Logic to select the correct DB shard based on a key (e.g., event_id or timestamp)\n",
    "    print(f\"Writing log event to DB: {log_event['event_id']}\")\n",
    "    # This is where your B+ tree index is implicitly used by the DB\n",
    "    return True\n",
    "\n",
    "# Kafka consumer setup\n",
    "KAFKA_BROKERS = ['kafka-broker-1:9092', 'kafka-broker-2:9092']\n",
    "consumer = KafkaConsumer(\n",
    "    'logs',\n",
    "    bootstrap_servers=KAFKA_BROKERS,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(\"Log worker listening for events...\")\n",
    "for message in consumer:\n",
    "    log_event = message.value\n",
    "    \n",
    "    # Write to the B+ tree-indexed database\n",
    "    if write_to_database(log_event):\n",
    "        # Populate the distributed LRU cache for recent logs\n",
    "        cache_key = f\"log:{log_event['event_id']}\"\n",
    "        set_with_lru_cache(cache_key, log_event, expiry=3600)\n",
    "        print(f\"Processed and cached log {log_event['event_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_service.py\n",
    "from flask import Flask, request, jsonify\n",
    "from utils import get_from_lru_cache\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Mock a database read function\n",
    "# In a real system, this would query your sharded database\n",
    "def query_database_for_log(log_id):\n",
    "    \"\"\"Simulates a DB query. Returns a log or None.\"\"\"\n",
    "    print(f\"Cache miss! Querying database for log ID: {log_id}\")\n",
    "    # This would perform an indexed lookup in your sharded B+ tree\n",
    "    # For a real system, you might use an ORM like SQLAlchemy\n",
    "    return {\"event_id\": log_id, \"message\": \"This is a log from the database\", \"timestamp\": \"...\"}\n",
    "\n",
    "@app.route('/query/<log_id>', methods=['GET'])\n",
    "def get_log(log_id):\n",
    "    # Step 1: Check the distributed LRU cache\n",
    "    log_event = get_from_lru_cache(f\"log:{log_id}\")\n",
    "    \n",
    "    if log_event:\n",
    "        print(\"Cache hit!\")\n",
    "        return jsonify(log_event), 200\n",
    "    \n",
    "    # Step 2: Cache miss, query the database\n",
    "    log_event = query_database_for_log(log_id)\n",
    "    \n",
    "    if log_event:\n",
    "        # Optionally, write the retrieved data back to the cache for future requests\n",
    "        # set_with_lru_cache(f\"log:{log_id}\", log_event)\n",
    "        return jsonify(log_event), 200\n",
    "    \n",
    "    return jsonify({\"error\": \"Log not found\"}), 404\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "CMD [\"python\", \"your_service.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.0.1\n",
    "    container_name: zookeeper\n",
    "    environment:\n",
    "      - ZOOKEEPER_CLIENT_PORT=2181\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.0.1\n",
    "    container_name: kafka\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      - KAFKA_BROKER_ID=1\n",
    "      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n",
    "      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "\n",
    "  redis-node-1:\n",
    "    image: redis:6.2.6\n",
    "    container_name: redis-node-1\n",
    "\n",
    "  redis-node-2:\n",
    "    image: redis:6.2.6\n",
    "    container_name: redis-node-2\n",
    "    \n",
    "  log-ingestion-api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: log-ingestion-api\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    environment:\n",
    "      - KAFKA_BROKERS=kafka:29092\n",
    "    depends_on:\n",
    "      - kafka\n",
    "\n",
    "  log-worker:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: log-worker\n",
    "    environment:\n",
    "      - KAFKA_BROKERS=kafka:29092\n",
    "      - REDIS_NODES=redis-node-1,redis-node-2\n",
    "    depends_on:\n",
    "      - kafka\n",
    "      - redis-node-1\n",
    "      - redis-node-2\n",
    "\n",
    "  query-service:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: query-service\n",
    "    ports:\n",
    "      - \"5001:5001\"\n",
    "    environment:\n",
    "      - REDIS_NODES=redis-node-1,redis-node-2\n",
    "    depends_on:\n",
    "      - redis-node-1\n",
    "      - redis-node-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a954ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06421d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc57fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
